{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UT-0ujfh8g-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a81e1122-8850-48ee-8789-33004ca04e7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.4.1\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark==3.4.1) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285391 sha256=bb6172fde0625f63f3767a1a89736db25419c86024f68a184f21cd130e243d38\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/b4/d8/38accc42606f6675165423e9f0236f8e825f6b6b6048d6743e\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.0\n",
            "    Uninstalling pyspark-3.5.0:\n",
            "      Successfully uninstalled pyspark-3.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "delta-spark 3.0.0 requires pyspark<3.6.0,>=3.5.0, but you have pyspark 3.4.1 which is incompatible.\n",
            "dataproc-spark-connect 0.7.5 requires pyspark[connect]~=3.5.1, but you have pyspark 3.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyspark-3.4.1\n",
            "Collecting delta-spark==2.4.0\n",
            "  Downloading delta_spark-2.4.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyspark<3.5.0,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from delta-spark==2.4.0) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from delta-spark==2.4.0) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.0.0->delta-spark==2.4.0) (3.23.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark<3.5.0,>=3.4.0->delta-spark==2.4.0) (0.10.9.7)\n",
            "Downloading delta_spark-2.4.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: delta-spark\n",
            "  Attempting uninstall: delta-spark\n",
            "    Found existing installation: delta-spark 3.0.0\n",
            "    Uninstalling delta-spark-3.0.0:\n",
            "      Successfully uninstalled delta-spark-3.0.0\n",
            "Successfully installed delta-spark-2.4.0\n"
          ]
        }
      ],
      "source": [
        "# Google Colab Setup (Run this only in Colab)\n",
        "!pip install pyspark==3.4.1\n",
        "!pip install delta-spark==2.4.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from delta import configure_spark_with_delta_pip\n",
        "from pyspark.sql.functions import col, when, sum as _sum\n",
        "\n",
        "builder = SparkSession.builder \\\n",
        "    .appName(\"ECommercePipeline\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
      ],
      "metadata": {
        "id": "K68mKQLsiinu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "orders_path = \"orders_delta\"\n",
        "customers_path = \"customers_delta\"\n",
        "products_path = \"products_delta\"\n",
        "\n",
        "\n",
        "orders_df = spark.read.csv(\"orders.csv\", header=True, inferSchema=True)\n",
        "orders_df.write.format(\"delta\").mode(\"overwrite\").save(orders_path)\n",
        "customers_df = spark.read.csv(\"customers.csv\", header=True, inferSchema=True)\n",
        "customers_df.write.format(\"delta\").mode(\"overwrite\").save(customers_path)\n",
        "\n",
        "products_df = spark.read.csv(\"products.csv\", header=True, inferSchema=True)\n",
        "products_df.write.format(\"delta\").mode(\"overwrite\").save(products_path)\n"
      ],
      "metadata": {
        "id": "DgXb_Qz5ipKb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "orders_df = spark.read.format(\"delta\").load(orders_path)\n",
        "\n",
        "\n",
        "orders_df.createOrReplaceTempView(\"orders\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    SELECT ProductID, SUM(Quantity * Price) AS TotalRevenue\n",
        "    FROM orders\n",
        "    WHERE Status = 'Delivered'\n",
        "    GROUP BY ProductID\n",
        "\"\"\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnXNwDpCi7HV",
        "outputId": "eb5766c9-ecad-4d9c-fb0a-7046eab98a3e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+\n",
            "|ProductID|TotalRevenue|\n",
            "+---------+------------+\n",
            "|    P1001|       75000|\n",
            "|    P1002|       50000|\n",
            "|    P1003|       30000|\n",
            "+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customers = spark.read.format(\"delta\").load(customers_path)\n",
        "orders = spark.read.format(\"delta\").load(orders_path)\n",
        "\n",
        "orders.join(customers, \"CustomerID\") \\\n",
        "    .filter(col(\"Status\") == \"Delivered\") \\\n",
        "    .groupBy(\"Region\") \\\n",
        "    .agg(_sum(col(\"Quantity\") * col(\"Price\")).alias(\"RegionRevenue\")) \\\n",
        "    .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KL5n-XQWi7KM",
        "outputId": "ae25196e-04ec-4f31-f2af-f81cb1f58cb1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------+\n",
            "|Region|RegionRevenue|\n",
            "+------+-------------+\n",
            "|  West|        30000|\n",
            "| North|       125000|\n",
            "+------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from delta.tables import DeltaTable\n",
        "\n",
        "delta_orders = DeltaTable.forPath(spark, orders_path)\n",
        "\n",
        "delta_orders.update(\n",
        "    condition=col(\"Status\") == \"Pending\",\n",
        "    set={\"Status\": \"'Cancelled'\"}\n",
        ")\n"
      ],
      "metadata": {
        "id": "DxYTp8fAlJHD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "new_return = spark.createDataFrame([\n",
        "    Row(OrderID=3006, CustomerID='C003', ProductID='P1003', Quantity=1, Price=30000, OrderDate='2024-05-06', Status='Returned')\n",
        "])\n",
        "\n",
        "delta_orders.alias(\"target\").merge(\n",
        "    new_return.alias(\"source\"),\n",
        "    \"target.OrderID = source.OrderID\"\n",
        ").whenNotMatchedInsertAll().execute()\n"
      ],
      "metadata": {
        "id": "ag_wGNb6lJJ7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_orders = spark.read.format(\"delta\").load(orders_path).dropna()\n",
        "cleaned_orders.write.format(\"delta\").mode(\"overwrite\").save(\"cleaned_orders_delta\")\n"
      ],
      "metadata": {
        "id": "DO2WE3XMlJMz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "products = spark.read.format(\"delta\").load(products_path)\n",
        "cleaned_orders = spark.read.format(\"delta\").load(\"cleaned_orders_delta\")\n",
        "\n",
        "cleaned_orders.join(products, \"ProductID\") \\\n",
        "    .withColumn(\"Revenue\", col(\"Quantity\") * col(\"Price\")) \\\n",
        "    .groupBy(\"Category\") \\\n",
        "    .agg(_sum(\"Revenue\").alias(\"TotalRevenue\")) \\\n",
        "    .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl7O37_0lTbu",
        "outputId": "ba6c9611-148f-4ad5-c32b-51ba7c46dff3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+\n",
            "|   Category|TotalRevenue|\n",
            "+-----------+------------+\n",
            "|Electronics|      285000|\n",
            "|Accessories|       30000|\n",
            "+-----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(orders_path).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD_qcEVulTL2",
        "outputId": "df42480c-66d3-4624-9b86-cc71397f2996"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|OrderID|CustomerID|ProductID|Quantity|Price| OrderDate|   Status|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "|   3001|      C001|    P1001|       1|75000|2024-05-01|Delivered|\n",
            "|   3002|      C002|    P1002|       2|50000|2024-05-02| Returned|\n",
            "|   3003|      C003|    P1003|       1|30000|2024-05-03|Delivered|\n",
            "|   3004|      C001|    P1002|       1|50000|2024-05-04|Delivered|\n",
            "|   3005|      C004|    P1004|       3|10000|2024-05-05|  Pending|\n",
            "+-------+----------+---------+--------+-----+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "old_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(orders_path)\n",
        "old_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(orders_path)\n"
      ],
      "metadata": {
        "id": "LbfYxcFRl1up"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
        "delta_orders.vacuum(retentionHours=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAt1Pn9Zl6Ia",
        "outputId": "d39d5544-e7bf-45eb-8de4-11fd391887e2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orders = spark.read.format(\"delta\").load(orders_path)\n",
        "\n",
        "orders.filter(\n",
        "    (col(\"Quantity\") <= 0) |\n",
        "    (col(\"Price\") <= 0) |\n",
        "    (col(\"OrderDate\").isNull())\n",
        ").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmn1dK0tl58L",
        "outputId": "08f14fa7-a65b-4203-9ad5-d3299a416168"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+--------+-----+---------+------+\n",
            "|OrderID|CustomerID|ProductID|Quantity|Price|OrderDate|Status|\n",
            "+-------+----------+---------+--------+-----+---------+------+\n",
            "+-------+----------+---------+--------+-----+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orders.withColumn(\n",
        "    \"OrderType\",\n",
        "    when(col(\"Status\") == \"Returned\", \"Return\").otherwise(\"Normal\")\n",
        ").select(\"OrderID\", \"Status\", \"OrderType\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOvRp9hkmoC9",
        "outputId": "149faa79-0220-4dbe-c46e-2cc4c5301c9c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+---------+\n",
            "|OrderID|   Status|OrderType|\n",
            "+-------+---------+---------+\n",
            "|   3001|Delivered|   Normal|\n",
            "|   3002| Returned|   Return|\n",
            "|   3003|Delivered|   Normal|\n",
            "|   3004|Delivered|   Normal|\n",
            "|   3005|  Pending|   Normal|\n",
            "+-------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}