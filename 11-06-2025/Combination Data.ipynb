{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6ce24ea-174b-4598-ab03-de602298ffc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, when, lit, isnull, count, current_date, months_between, to_date\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"EmployeeProjectAnalysis\").getOrCreate()\n",
    "employee_data = [\n",
    "    (\"Ananya\", \"HR\", 52000),\n",
    "    (\"Rahul\", \"Engineering\", 65000),\n",
    "    (\"Priya\", \"Engineering\", 60000),\n",
    "    (\"Zoya\", \"Marketing\", 48000),\n",
    "    (\"Karan\", \"HR\", 53000),\n",
    "    (\"Naveen\", \"Engineering\", 70000),\n",
    "    (\"Fatima\", \"Marketing\", 45000)\n",
    "]\n",
    "columns_emp = [\"Name\", \"Department\", \"Salary\"]\n",
    "df_emp = spark.createDataFrame(employee_data, columns_emp)\n",
    "performance_data = [\n",
    "    (\"Ananya\", 2023, 4.5),\n",
    "    (\"Rahul\", 2023, 4.9),\n",
    "    (\"Priya\", 2023, 4.3),\n",
    "    (\"Zoya\", 2023, 3.8),\n",
    "    (\"Karan\", 2023, 4.1),\n",
    "    (\"Naveen\", 2023, 4.7),\n",
    "    (\"Fatima\", 2023, 3.9)\n",
    "]\n",
    "columns_perf = [\"Name\", \"Year\", \"Rating\"]\n",
    "df_perf = spark.createDataFrame(performance_data, columns_perf)\n",
    "project_data = [\n",
    "    (\"Ananya\", \"HR Portal\", 120),\n",
    "    (\"Rahul\", \"Data Platform\", 200),\n",
    "    (\"Priya\", \"Data Platform\", 180),\n",
    "    (\"Zoya\", \"Campaign Tracker\", 100),\n",
    "    (\"Karan\", \"HR Portal\", 130),\n",
    "    (\"Naveen\", \"ML Pipeline\", 220),\n",
    "    (\"Fatima\", \"Campaign Tracker\", 90)\n",
    "]\n",
    "columns_proj = [\"Name\", \"Project\", \"HoursWorked\"]\n",
    "df_proj = spark.createDataFrame(project_data, columns_proj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "132594b8-dadc-46fb-9278-dde3719f9c3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " 1. Joins and Advanced Aggregations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b669ce49-caca-4f1e-bc34-840a30233a28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+------+----------------+-----------+\n|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|\n+------+-----------+------+----+------+----------------+-----------+\n|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|\n| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200|\n| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|\n|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|\n| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|\n|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|\n|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|\n+------+-----------+------+----+------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_merged = df_emp.join(df_perf, \"Name\").join(df_proj, \"Name\")\n",
    "df_merged.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03f98d64-df4a-4f65-9778-f10d2c742016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n| Department|TotalHours|\n+-----------+----------+\n|         HR|       250|\n|Engineering|       600|\n|  Marketing|       190|\n+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_merged.groupBy(\"Department\").sum(\"HoursWorked\").withColumnRenamed(\"sum(HoursWorked)\", \"TotalHours\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5cca52a-0ba8-41b2-852a-d2ae41dd5219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n|         Project|         AvgRating|\n+----------------+------------------+\n|       HR Portal|               4.3|\n|   Data Platform|               4.6|\n|Campaign Tracker|3.8499999999999996|\n|     ML Pipeline|               4.7|\n+----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_merged.groupBy(\"Project\").agg(avg(\"Rating\").alias(\"AvgRating\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acd87b87-a137-4afa-bd16-77c7e812f018",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Handling Missing Data (introduce some manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cd06656-e3f5-4c37-aeab-77cd0af9d2b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n|  Name|Year|Rating|\n+------+----+------+\n|Ananya|2023|   4.5|\n| Rahul|2023|   4.9|\n| Priya|2023|   4.3|\n|  Zoya|2023|   3.8|\n| Karan|2023|   4.1|\n|Naveen|2023|   4.7|\n|Fatima|2023|   3.9|\n| Dummy|2023|  NULL|\n+------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"Rating\", FloatType(), True),  \n",
    "    FloatType\n",
    "])\n",
    "\n",
    "new_row = spark.createDataFrame([(\"Dummy\", 2023, None)], schema=schema)\n",
    "df_perf_null = df_perf.unionByName(new_row)\n",
    "df_perf_null.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "327c13e4-0cbe-4f22-a923-da874f9cbf0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n| Name|Year|Rating|\n+-----+----+------+\n|Dummy|2023|  NULL|\n+-----+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_perf_null.filter(col(\"Rating\").isNull()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b80f5f7-6ed5-458a-9c7b-7c44bd13cd23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------------+\n|  Name| Department|RatingFilled|\n+------+-----------+------------+\n|Ananya|         HR|         4.5|\n| Rahul|Engineering|         4.9|\n| Priya|Engineering|         4.3|\n|  Zoya|  Marketing|         3.8|\n| Karan|         HR|         4.1|\n|Naveen|Engineering|         4.7|\n|Fatima|  Marketing|         3.9|\n| Dummy|       NULL|        NULL|\n+------+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_temp = df_perf_null.join(df_emp, \"Name\", \"left\")\n",
    "dept_avg = df_temp.groupBy(\"Department\").agg(avg(\"Rating\").alias(\"DeptAvg\"))\n",
    "df_temp = df_temp.join(dept_avg, \"Department\", \"left\")\n",
    "df_filled = df_temp.withColumn(\n",
    "    \"RatingFilled\",\n",
    "    when(col(\"Rating\").isNull(), col(\"DeptAvg\")).otherwise(col(\"Rating\"))\n",
    ").select(\"Name\", \"Department\", \"RatingFilled\")\n",
    "\n",
    "df_filled.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6943da8-9587-4c58-8f5b-07b8fbbd0f1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Built-In Functions and UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c229c7cb-90f0-4759-be6b-a0549408cd50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+-------------------+\n|  Name|Year|Rating|PerformanceCategory|\n+------+----+------+-------------------+\n|Ananya|2023|   4.5|               Good|\n| Rahul|2023|   4.9|          Excellent|\n| Priya|2023|   4.3|               Good|\n|  Zoya|2023|   3.8|            Average|\n| Karan|2023|   4.1|               Good|\n|Naveen|2023|   4.7|          Excellent|\n|Fatima|2023|   3.9|            Average|\n+------+----+------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_cat = df_perf.withColumn(\n",
    "    \"PerformanceCategory\",\n",
    "    when(col(\"Rating\") >= 4.7, \"Excellent\")\n",
    "    .when(col(\"Rating\") >= 4.0, \"Good\")\n",
    "    .otherwise(\"Average\")\n",
    ")\n",
    "df_cat.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9d33e60-e722-45cf-b425-384e9d74468b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+-----------+-----+\n|  Name|         Project|HoursWorked|Bonus|\n+------+----------------+-----------+-----+\n|Ananya|       HR Portal|        120| 5000|\n| Rahul|   Data Platform|        200| 5000|\n| Priya|   Data Platform|        180| 5000|\n|  Zoya|Campaign Tracker|        100| 5000|\n| Karan|       HR Portal|        130| 5000|\n|Naveen|     ML Pipeline|        220|10000|\n|Fatima|Campaign Tracker|         90| 5000|\n+------+----------------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def bonus_udf(hours):\n",
    "    return 10000 if hours > 200 else 5000\n",
    "\n",
    "bonus_func = udf(bonus_udf, IntegerType())\n",
    "df_with_bonus = df_proj.withColumn(\"Bonus\", bonus_func(col(\"HoursWorked\")))\n",
    "df_with_bonus.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98f9ef38-826d-4c1a-b6b4-698a3a449550",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Date and Time Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3958602-363c-4463-8b3b-b185515d5ca0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------+\n|  Name|  JoinDate|MonthsWorked|\n+------+----------+------------+\n|Ananya|2021-06-01|          48|\n| Rahul|2021-06-01|          48|\n| Priya|2021-06-01|          48|\n|  Zoya|2021-06-01|          48|\n| Karan|2021-06-01|          48|\n|Naveen|2021-06-01|          48|\n|Fatima|2021-06-01|          48|\n+------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import months_between\n",
    "\n",
    "df_emp_date = df_emp.withColumn(\"JoinDate\", to_date(lit(\"2021-06-01\")))\n",
    "df_emp_date = df_emp_date.withColumn(\"MonthsWorked\", months_between(current_date(), col(\"JoinDate\")).cast(\"int\"))\n",
    "df_emp_date.select(\"Name\", \"JoinDate\", \"MonthsWorked\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9da76fa4-ed24-40af-8807-f30e780d075d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emp_date.filter(col(\"JoinDate\") < \"2022-01-01\").count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b7acd09-ca45-4f10-ae99-5fa27dd63b32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09ca61b3-fb68-4e83-a11a-d0da3013760a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+\n|  Name| Department|Salary|\n+------+-----------+------+\n|Ananya|         HR| 52000|\n| Rahul|Engineering| 65000|\n| Priya|Engineering| 60000|\n|  Zoya|  Marketing| 48000|\n| Karan|         HR| 53000|\n|Naveen|Engineering| 70000|\n|Fatima|  Marketing| 45000|\n| Meena|         HR| 48000|\n|   Raj|  Marketing| 51000|\n+------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "extra_employees = [\n",
    "    (\"Meena\", \"HR\", 48000),\n",
    "    (\"Raj\", \"Marketing\", 51000)\n",
    "]\n",
    "df_extra = spark.createDataFrame(extra_employees, columns_emp)\n",
    "df_union = df_emp.unionByName(df_extra)\n",
    "df_union.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "826be7bd-7805-464c-bd4b-7e7c02559018",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9bf2952-b1e8-4b13-9386-838848244788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = df_emp.join(df_perf, \"Name\").join(df_proj, \"Name\")\n",
    "df_final.write.mode(\"overwrite\").parquet(\"/tmp/final_employee_dataset\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Combination Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}